{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for cauchy multiplication not found. Install by going to extensions/cauchy/ and running `python setup.py install`. This should speed up end-to-end training by 10-50%\n",
      "Falling back on slow Cauchy kernel. Install at least one of pykeops or the CUDA extension for efficiency.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import importlib.util\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "module_path = r\"D:/timeseries/package/state-spaces-simple/src/models/sequence/ss/standalone/s4.py\"\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"S4\", module_path)\n",
    "S4 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(S4)\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('D:/timeseries/data/data_6.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SYMBOL</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>SIZE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>561.02</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>560.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>558.19</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>558.19</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>4:00:00</td>\n",
       "      <td>557.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86035742</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>19:08:52</td>\n",
       "      <td>46.48</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86035743</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>19:12:03</td>\n",
       "      <td>46.48</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86035744</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>19:37:51</td>\n",
       "      <td>46.57</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86035745</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>19:52:13</td>\n",
       "      <td>46.57</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86035746</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>19:58:08</td>\n",
       "      <td>46.57</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86035747 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SYMBOL        DATE      TIME   PRICE  SIZE\n",
       "0          AAPL  2014-01-02   4:00:00  561.02     9\n",
       "1          AAPL  2014-01-02   4:00:00  560.00     5\n",
       "2          AAPL  2014-01-02   4:00:00  558.19    86\n",
       "3          AAPL  2014-01-02   4:00:00  558.19    64\n",
       "4          AAPL  2014-01-02   4:00:00  557.00     5\n",
       "...         ...         ...       ...     ...   ...\n",
       "86035742   MSFT  2014-12-31  19:08:52   46.48    50\n",
       "86035743   MSFT  2014-12-31  19:12:03   46.48     7\n",
       "86035744   MSFT  2014-12-31  19:37:51   46.57   100\n",
       "86035745   MSFT  2014-12-31  19:52:13   46.57   100\n",
       "86035746   MSFT  2014-12-31  19:58:08   46.57   100\n",
       "\n",
       "[86035747 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=list(set(data[\"SYMBOL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(sel,names):\n",
    "  \n",
    "  datamain=sel.query(\"SYMBOL=='%s'\"%(names[0]))\n",
    "  datamain=datamain[[\"DATE\",\"PRICE\"]]\n",
    "  datamain.drop_duplicates(subset=[\"DATE\"], keep='first', inplace=True)\n",
    "  datamain=datamain.sort_values(by=\"DATE\")\n",
    "  s=names[0]+\"_price\"\n",
    "  s2=names[0]+\"_size\"\n",
    "  # datamain[\"DATE\"]=pd.to_datetime(datamain[\"DATE\"])\n",
    "  datamain=datamain.rename(columns={\"PRICE\":s})\n",
    "  # datamain[s] = pd.to_numeric(datamain[\"PRICE\"])\n",
    "  # datamain[s2] = pd.to_numeric(datamain[\"SIZE\"])\n",
    "\n",
    "\n",
    "  for name in names:\n",
    "    if name==names[0]:\n",
    "      continue\n",
    "    data=sel.query(\"SYMBOL=='%s'\"%name)\n",
    "\n",
    "    data=data[[\"DATE\",\"PRICE\"]]\n",
    "    data.drop_duplicates(subset=[\"DATE\"], keep='first', inplace=True)\n",
    "    \n",
    "    # data[\"PRICE\"]=pd.to_datetime(data[\"PRICE\"])\n",
    "    # data[\"Close_\"] = pd.to_numeric(data[\"Close_\"])\n",
    " \n",
    "    data=data.rename(columns={\"PRICE\": \"%s_price\"%name})\n",
    "\n",
    "\n",
    "    datamain=pd.merge(datamain,data,on=\"DATE\", how=\"outer\")\n",
    "  return datamain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=transform_data(data,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data=data.astype(float)\n",
    "    mean_list=[]\n",
    "    std_list=[]\n",
    "    for i in data.columns:\n",
    "        try:\n",
    "          # mean=data[i].mean()\n",
    "          mean=0\n",
    "        except:\n",
    "          print(data[i],i)\n",
    "          break\n",
    "        std=1\n",
    "        std=data[i].std()\n",
    "        data[i]=(data[i]-mean)/std\n",
    "        # for j in range(len(data[i])):\n",
    "          # if data[i][j]!=0:\n",
    "          #   first=data[i][j]\n",
    "          #   data[i]=data[i]/first+5\n",
    "          #   break\n",
    "\n",
    "        \n",
    "        mean_list.append(mean)\n",
    "        std_list.append(std)   \n",
    "    # return data,first\n",
    "    return data,mean_list,std_list\n",
    "\n",
    "def get_mask(data):\n",
    "    \"\"\"\n",
    "    data should in the form of pd.df\n",
    "    gen a tenor with 0 and 1 to represent missing data\n",
    "    \"\"\"\n",
    "    mask = ~data.isnan().values\n",
    " \n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.float32)\n",
    "    \n",
    "    mask_tensor= mask_tensor.transpose(0,1)\n",
    "    return mask_tensor\n",
    "\n",
    "def mape(A,F,maskf_sub):\n",
    "  sum=0\n",
    "  length=0\n",
    "  for i in range(len(A)):\n",
    "   \n",
    "    if maskf_sub[i]!=0:\n",
    "      sum+=abs(A[i] - F[i]) / abs(A[i])\n",
    "      length+=1\n",
    "  if length>0:\n",
    "    return 100/length*sum\n",
    "  \n",
    "  return 0\n",
    "def smape(A, F,maskf_sub):\n",
    "  sum=0\n",
    "  length=0\n",
    "  for i in range(len(A)):\n",
    "    if maskf_sub[i]!=0:\n",
    "      sum+=2 * abs(F[i] - A[i]) / (abs(A[i]) + abs(F[i]))\n",
    "      length+=1\n",
    "  if length>0:\n",
    "\n",
    "    return 100/length * sum\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_inputs(data, context_length, prediction_length):\n",
    "    num_days, num_products = data.shape\n",
    "    num_samples = num_days - context_length - prediction_length + 1\n",
    "   \n",
    "\n",
    "    samples = torch.zeros((num_samples, context_length,num_products))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        samples[i,:,:] = data[i:i+context_length]\n",
    "\n",
    "    return samples,num_samples\n",
    "def create_targets(data,context_length,prediction_length):\n",
    "    num_days, num_products = data.shape\n",
    "    num_samples = num_days - context_length - prediction_length + 1\n",
    "    \n",
    "\n",
    "    targets = torch.zeros((num_samples, prediction_length,num_products))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        targets[i, :,:] = data[i+context_length:i+prediction_length+context_length]\n",
    "\n",
    "    return targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_val(data,prediction_period,batchs,context_length,col_len):\n",
    " \n",
    "    whole,m,std=normalize(data.iloc[:,1:])\n",
    "    # whole,m,std=normalize(data)\n",
    "    whole=torch.tensor(whole.values)\n",
    "    # whole=whole.transpose(0,1)\n",
    "   \n",
    "    inputs,period=create_inputs(whole,context_length,prediction_period)\n",
    "    target=create_targets(whole,context_length,prediction_period)\n",
    "    inputs=inputs.reshape(period,1,col_len*context_length)\n",
    "    target=target.reshape(period,1,prediction_period*col_len)\n",
    "    # print(inputs.shape)\n",
    "    # train_input=inputs[:period-prediction_period].transpose(0,1)\n",
    "    # test_input=inputs[:period-prediction_period].transpose(0,1)\n",
    "    # train_target=target[:period-prediction_period].transpose(0,1)\n",
    "    # test_target=target[period-prediction_period:].transpose(0,1)\n",
    "    train_input=inputs[:period-prediction_period]\n",
    "    test_input=inputs[-1:]\n",
    "    train_target=target[:period-prediction_period]\n",
    "    test_target=target[-1:]\n",
    "    print(test_input.shape,test_target.shape)\n",
    "    \n",
    "    traindict={'target':train_target,'input':train_input}\n",
    "    testdict={'target':test_target,'input':test_input}\n",
    "    train=Dataset.from_dict(traindict)\n",
    "    train=train.with_format('torch')\n",
    "    test=Dataset.from_dict(testdict)\n",
    "    test=test.with_format('torch')\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batchs, shuffle=False)\n",
    "    test_loader = DataLoader(test, batch_size=batchs, shuffle=False)\n",
    "    return train_loader, test_loader,m,std\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_input, \n",
    "        d_output=10, \n",
    "        d_model=256, \n",
    "        n_layers=4, \n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4.S4(\n",
    "                    d_model=d_model, \n",
    "                    l_max=1024, \n",
    "                    bidirectional=True,\n",
    "                    postact='glu',\n",
    "                    dropout=dropout, \n",
    "                    transposed=True,\n",
    "                )\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(nn.Dropout2d(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "        \n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "            \n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch,train,ct,pt,col_len):\n",
    "    model = S4Model(\n",
    "    d_input=col_len*ct, \n",
    "    d_output=pt*col_len, \n",
    "    d_model=64, \n",
    "    n_layers=4, \n",
    "\n",
    "    dropout=0.1,\n",
    "    prenorm=False\n",
    "    )\n",
    "    device='cpu'\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.001,)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epoch):\n",
    "        for ind,batch in enumerate(train):\n",
    "            optimizer.zero_grad()\n",
    "            target=batch['target']\n",
    "            inputs=batch['input']\n",
    "            # target_mask = ~torch.isnan(target)\n",
    "            # target_mask = target_mask.view(target.shape[0],target.shape[1],target.shape[2])\n",
    "            # valid_target = target[target_mask]\n",
    "            # input_mask=target_mask.reshape()\n",
    "            # valid_inputs = inputs[target_mask]\n",
    "        \n",
    "            \n",
    "            # inputs=Variable(inputs,requires_grad=True)\n",
    "            # input_mask = ~torch.isnan(inputs)\n",
    "            # input_mask=input_mask.reshape(inputs.shape[0],inputs.shape[1],inputs.shape[2])\n",
    "            # print(input_mask.shape)\n",
    "            # valid_inputs = inputs[input_mask]\n",
    "            # print(valid_inputs.shape)\n",
    "            \n",
    "            # inputs=inputs.nan_to_num()\n",
    "            inputs=Variable(inputs,requires_grad=True)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            # output_mask=target_mask.reshape(outputs.shape[0],outputs.shape[1])\n",
    "            criterion = nn.MSELoss()\n",
    "            # valid_outputs=outputs[output_mask]\n",
    "            # print(valid_outputs)\n",
    "            # break\n",
    "            loss =criterion(outputs,target)\n",
    "            # loss =criterion(valid_outputs,valid_target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test,pt,col_len):\n",
    "    pred=[]\n",
    "    for ind,batch in enumerate(test):\n",
    "        inputs=batch['input']\n",
    "        inputs=inputs.nan_to_num()\n",
    "        print(inputs,inputs.shape)\n",
    "        out=model(inputs)\n",
    "        out=out.reshape(pt,col_len)\n",
    "        out=out.T\n",
    "        pred=pred+list(out)\n",
    "        \n",
    "    return pred\n",
    "            \n",
    "def denormalize(data,mean,std):\n",
    "    for i in range(len(data)):\n",
    "        print(data[i],std[i],mean[i])\n",
    "        data[i]=data[i]*std[i]+mean[i]\n",
    "    return data       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S4_run(data,pred_length,batch_size,context_l,epoch,col_len):\n",
    "    train,test,m,std=split_train_val(data,pred_length,batch_size,context_l,col_len)\n",
    "    model=train_model(epoch,train,context_l,pred_length,col_len)\n",
    "    pred=predict(model,test,pred_length,col_len)\n",
    "    denormalize_pred=denormalize(pred,m,std)\n",
    "    return model,pred,denormalize_pred,test,m,std\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 6]) torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([50, 1, 3])) that is different to the input size (torch.Size([50, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\Users\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([49, 1, 3])) that is different to the input size (torch.Size([49, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.81801986694336\n",
      "24.98859214782715\n",
      "18.627809524536133\n",
      "14.692171096801758\n",
      "11.717824935913086\n",
      "9.26452922821045\n",
      "7.213115215301514\n",
      "5.540320873260498\n",
      "4.213038444519043\n",
      "3.1828267574310303\n",
      "2.4053151607513428\n",
      "1.8364379405975342\n",
      "1.4290872812271118\n",
      "1.1391205787658691\n",
      "0.9306294918060303\n",
      "0.7767571210861206\n",
      "0.659277617931366\n",
      "0.5658623576164246\n",
      "0.48564326763153076\n",
      "0.41251298785209656\n",
      "0.34935280680656433\n",
      "0.3005467355251312\n",
      "0.265882670879364\n",
      "0.24185191094875336\n",
      "0.22467705607414246\n",
      "tensor([[[ 0.5069,  1.9837, 12.2360,  0.5060,  1.9692, 12.1183]]]) torch.Size([1, 1, 6])\n",
      "tensor([0.5391], grad_fn=<UnbindBackward0>) 225.30781514157803 0\n",
      "tensor([1.9446], grad_fn=<UnbindBackward0>) 268.3608860827003 0\n",
      "tensor([11.4354], grad_fn=<UnbindBackward0>) 3.9097805695284733 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get=S4_run(new_data,1,50,2,50,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mape(A,F,maskf_sub):\n",
    "  sum=0\n",
    "  length=0\n",
    "  for i in range(len(A)):\n",
    "    # print(A[i],\"A\",maskf_sub[i],\"ma\")\n",
    "    if not torch.isnan(A[i]):\n",
    "    # if maskf_sub[i]!=0:\n",
    "      sum+=abs(A[i] - F[i]) / abs(A[i])\n",
    "      length+=1\n",
    "  if length>0:\n",
    "    return 100/length*sum\n",
    "  \n",
    "  return 0\n",
    "def smape(A, F,maskf_sub):\n",
    "  sum=0\n",
    "  length=0\n",
    "  for i in range(len(A)):\n",
    "    if not torch.isnan(A[i]):\n",
    "    # if maskf_sub[i]!=0:\n",
    "      sum+=2 * abs(F[i] - A[i]) / (abs(A[i]) + abs(F[i]))\n",
    "      length+=1\n",
    "  if length>0:\n",
    "\n",
    "    return 100/length * sum\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False,  True,  True])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5010]) 225.30781514157803 0\n",
      "tensor([1.9820]) 268.3608860827003 0\n",
      "tensor([12.0339]) 3.9097805695284733 0\n"
     ]
    }
   ],
   "source": [
    "smpl=[]\n",
    "mpl=[]\n",
    "m=get[4]\n",
    "std=get[5]\n",
    "act=next(iter(get[3]))\n",
    "act=(act['target']).reshape(1,3)\n",
    "act=act.T\n",
    "deno_pred=get[2]\n",
    "act=denormalize(act,m,std)\n",
    "act_mask=(~torch.isnan(act))\n",
    "act_mask=act_mask.reshape(3*1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    mp=mape(act[i],deno_pred[i],act_mask)\n",
    "    smp=smape(act[i],deno_pred[i],act_mask)\n",
    "    \n",
    "    smpl.append(float(smp))\n",
    "    mpl.append(float(mp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([121.4694], grad_fn=<CopySlices>),\n",
       " tensor([521.8474], grad_fn=<CopySlices>),\n",
       " tensor([44.7098], grad_fn=<CopySlices>)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(deno_pred)):\n",
    "    for j in range(len(deno_pred[i])):\n",
    "        deno_pred[i][j]=float(deno_pred[i][j])\n",
    "deno_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred=[]\n",
    "for i in range(1):\n",
    "    new_pred.append([])\n",
    "    for j in range(3):\n",
    "        new_pred[i].append(float(deno_pred[j][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act=act.transpose(0,1)\n",
    "act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:/timeseries/result/temp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\timeseries\\S4.ipynb Cell 19\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/timeseries/S4.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mD:/timeseries/result/temp_2.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/timeseries/S4.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m res2\u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m:names,\u001b[39m'\u001b[39m\u001b[39mday1/1_pred\u001b[39m\u001b[39m'\u001b[39m:act[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mday1/1_actu\u001b[39m\u001b[39m'\u001b[39m:new_pred[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/timeseries/S4.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                        })\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/timeseries/S4.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m res2\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39mD:/timeseries/result/temp.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Users\\lib\\site-packages\\pandas\\core\\generic.py:3563\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3552\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3554\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3555\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3556\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3560\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3561\u001b[0m )\n\u001b[1;32m-> 3563\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3564\u001b[0m     path_or_buf,\n\u001b[0;32m   3565\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   3566\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3567\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3568\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3569\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3570\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3571\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3572\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3573\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3574\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3575\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3576\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3577\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3578\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3579\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3580\u001b[0m )\n",
      "File \u001b[1;32md:\\Users\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32md:\\Users\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32md:\\Users\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:/timeseries/result/temp.csv'"
     ]
    }
   ],
   "source": [
    "# act=act.T\n",
    "result= pd.DataFrame({'S4_mape':mpl,'S4_smape':smpl})\n",
    "result.to_csv('D:/timeseries/result/temp_2.csv')\n",
    "res2= pd.DataFrame({'name':names,'day1/1_pred':act[0], 'day1/1_actu':new_pred[0]\n",
    "                       })\n",
    "res2.to_csv('D:/timeseries/result/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act=act.T\n",
    "result= pd.DataFrame({'S4_mape':mpl,'S4_smape':smpl})\n",
    "result.to_csv('D:/timeseries/result/temp_2.csv')\n",
    "res2= pd.DataFrame({'name':names,'day1/7_pred':act[0], 'day1/7_actu':new_pred[0],'day2/7_pred':act[1], \n",
    "                    'day2/7_actu':new_pred[1],'day3/7_pred':act[2], 'day3/7_actu':new_pred[2],'day4/7_pred':act[3], 'day4/7_actu':new_pred[3]\n",
    "                    ,'day5/7_pred':act[4], 'day5/7_actu':new_pred[4],'day6/7_pred':act[5], 'day6/7_actu':new_pred[5],\n",
    "                    'day7/7_pred':act[6], 'day7/7_actu':new_pred[6]\n",
    "                       })\n",
    "res2.to_csv('D:/timeseries/result/temp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
