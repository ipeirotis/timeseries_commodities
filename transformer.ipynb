{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict\n",
    "from gluonts.transform import AddTimeFeatures\n",
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./data/data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=['DJUBSWH_x', 'GMECN09', 'DJUBCST', 'SWIX23H',\n",
    "       'GMESC15', 'NPN416E', 'NPN206E', 'CDOM2UM', 'WTLM3UM', 'NPFL09E',\n",
    "       'SGENCHT', 'EEXHR19', 'SPGPGHE', 'SGGYEXR', 'DCIENTR', 'GMESA10',\n",
    "       'NPS404E', 'OIL45SC', 'RJEFNET', 'LCP15MT', 'NPDW17E', 'MLCXDJX',\n",
    "       'GMESI23', 'NPN104E', 'DJUBSWH_y']\n",
    "name=name[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(A,F):\n",
    "  sum=0\n",
    "  \n",
    "  for i in range(len(A)):\n",
    "   \n",
    "\n",
    "    sum+=abs(A[i] - F[i]) / abs(A[i])\n",
    "  return 100/len(A)*sum\n",
    "def smape(A, F):\n",
    "  sum=0\n",
    "  for i in range(len(A)):\n",
    "    sum+=2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n",
    "\n",
    "  return 100/len(A) * sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324, 202)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(data,period,prediction_period,inputs,batchs,context_length):\n",
    "    dates = pd.date_range(data.iloc[0,0], periods=period, freq='D')\n",
    "\n",
    "\n",
    "    ind=pd.PeriodIndex(data=dates,freq='D')\n",
    "    time_feature=ind.dayofyear.astype(float).values - 1\n",
    "\n",
    "    past=torch.tensor([[time_feature[:period-prediction_period]]]*inputs)\n",
    "    past=past.transpose(1,2)\n",
    "    future=torch.tensor([[time_feature[period-prediction_period:period]]]*inputs)\n",
    "    future=future.transpose(1,2)\n",
    "\n",
    "    static=(torch.zeros((inputs,1))).to(torch.float32)\n",
    "    no=torch.arange(1,inputs+1,1)\n",
    "    no=no.unsqueeze(1)\n",
    "    no=no.to(torch.long)\n",
    "    maskp=torch.ones((inputs,period))\n",
    "    # maskf=torch.ones((inputs,prediction_period))\n",
    "    # for i in range(inputs):\n",
    "    #     for j in range(period):\n",
    "\n",
    "    #         if data.iloc[j,i]==\"\":\n",
    "    #             maskp[i][j]=0\n",
    "    maskf=maskp[:,period-prediction_period:]\n",
    "    maskp=maskp[:,:period-prediction_period]\n",
    "    \n",
    "   \n",
    "    whole=torch.tensor(data[name].values)\n",
    "    input=whole[:period-prediction_period].transpose(0,1)\n",
    "    target=whole[period-prediction_period:].transpose(0,1)\n",
    "    traindict={'target':target,'input':input,'past':past,'future':future,'maskp':maskp,'maskf':maskf,'sta':no,'real':static}\n",
    "    train=Dataset.from_dict(traindict)\n",
    "    train=train.with_format('torch')\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batchs, shuffle=False)\n",
    "    lags_sequence = get_lags_for_frequency('1D')\n",
    "    lags_sequence[15]=period-context_length-prediction_period\n",
    "\n",
    "\n",
    "    prediction_length = prediction_period\n",
    "    \n",
    "    input_size = 1\n",
    "    config = TimeSeriesTransformerConfig(\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "        input_size=input_size,\n",
    "        encoder_layers=4,\n",
    "        decoder_layers=4,\n",
    "        d_model=22,\n",
    "        num_static_categorical_features= 1,\n",
    "        num_static_real_features= 1,\n",
    "        num_time_features=1,\n",
    "        cardinality=[inputs+1],\n",
    "        lags_sequence=lags_sequence[:16]\n",
    "    \n",
    "        \n",
    "        \n",
    "    )\n",
    "    model = TimeSeriesTransformerForPrediction(config)\n",
    "    \n",
    "    learning_rate = 0.001  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(101):\n",
    "        for ind,batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                static_categorical_features=batch[\"sta\"]\n",
    "                if config.num_static_categorical_features > 0\n",
    "                else None,\n",
    "                static_real_features=batch[\"real\"]\n",
    "                if config.num_static_real_features > 0\n",
    "                else None,\n",
    "                past_time_features=batch[\"past\"],\n",
    "                past_values=batch[\"input\"],\n",
    "                future_time_features=batch[\"future\"],\n",
    "                future_values=batch[\"target\"],\n",
    "                past_observed_mask=batch[\"maskp\"],\n",
    "                future_observed_mask=batch[\"maskf\"],\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "    forecasts = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        outputs = model.generate(\n",
    "            static_categorical_features=batch[\"sta\"]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"real\"]\n",
    "            if config.num_static_real_features > 0\n",
    "            else None,\n",
    "            past_time_features=batch[\"past\"],\n",
    "            past_values=batch[\"input\"],\n",
    "            future_time_features=batch[\"future\"],\n",
    "            past_observed_mask=batch[\"maskp\"],\n",
    "        )\n",
    "        forecasts.append(outputs.sequences.cpu().numpy())\n",
    "    forecasts = np.vstack(forecasts)\n",
    "    foremean=np.mean(forecasts,1)\n",
    "    test=train['target']\n",
    "\n",
    "    mapel=[]\n",
    "    smapel=[]\n",
    "    for i in range(inputs):\n",
    "        m=mape(foremean[i],test[i])\n",
    "        mapel.append(float(m))\n",
    "        sm=smape(foremean[i],test[i])\n",
    "        smapel.append(float(sm))\n",
    "    return foremean,test,mapel,smapel\n",
    "# get=transformer(data,data.shape[0],7,data.shape[1],25,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.93638801574707\n",
      "4.075043678283691\n",
      "3.9512970447540283\n"
     ]
    }
   ],
   "source": [
    "get=transformer(data,data.shape[0],3,200,30,6)\n",
    "\n",
    "# result= pd.DataFrame({'trans_mape':mapel,'trans_smape':smapel})\n",
    "# result.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2011/8/22'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= pd.DataFrame({'trans_mape':get[2],'trans_smape':get[3]})\n",
    "res.to_csv('D:/timeseries/result/temp2.csv')\n",
    "\n",
    "# result= pd.DataFrame({'name':name,'day1_pred':get[0][:,0], 'day1_actu':get[1][:,0]\n",
    "#                        })\n",
    "# result.to_csv('D:/timeseries/result/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\lib\\site-packages\\plotly_resampler\\aggregation\\aggregators.py:25: UserWarning: Could not import lttbc; will use a (slower) python alternative.\n",
      "  warnings.warn(\"Could not import lttbc; will use a (slower) python alternative.\")\n"
     ]
    }
   ],
   "source": [
    "from neuralprophet import NeuralProphet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.base.data import handle_missing\n",
    "\n",
    "def ne_prophet(train_data,test_data,length,fre):\n",
    "  model = NeuralProphet(\n",
    "    n_lags=5,\n",
    "    n_changepoints=20,\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "    drop_missing=True)\n",
    "  metrics = model.fit(train_data, freq=fre) \n",
    "  future = model.make_future_dataframe(train_data, periods=len(test_data), n_historic_predictions=len(train_data)) \n",
    "  forecast = model.predict(future)\n",
    "\n",
    "  model.plot(forecast)\n",
    "  forecast=forecast[length:]\n",
    "  \n",
    "  sma=smape(test_data,list(forecast[\"yhat1\"]))\n",
    "  ma=mape(test_data,list(forecast[\"yhat1\"]))\n",
    "  return forecast,sma,ma\n",
    "\n",
    "\n",
    "\n",
    "def run(datas,j):\n",
    "  # mape_list=[]\n",
    "  smape_list=[]\n",
    "  smape_list_np=[]\n",
    "  for data in datas:\n",
    "\n",
    "  \n",
    "    test_data_time=data[\"Date_\"][-j:]\n",
    "    train_data=data[:-j]\n",
    "    test_data=list(data[\"Close_\"][-j:])\n",
    "   \n",
    "    train_data.columns=[\"ds\",\"y\"]\n",
    "    \n",
    "   \n",
    "    try:\n",
    "      \n",
    "    \n",
    "      \n",
    "      resultn,sman,smap=ne_prophet(train_data,test_data,len(data)-j,\"D\")\n",
    "    \n",
    "      smape_list.append(smap)\n",
    "      smape_list_np.append(sman)\n",
    "    except:\n",
    "      continue\n",
    "  return smape_list,smape_list_np\n",
    "\n",
    "\n",
    "\n",
    "smape_list=[]\n",
    "smape_list_np=[]\n",
    "result=[]\n",
    "test=[]\n",
    "for i in range(0,200):\n",
    "  use=data[['ds',name[i]]]\n",
    "  use=use.rename(columns={name[i]:'y'})\n",
    "\n",
    " \n",
    "  train_data=use[:-7]\n",
    "  test_data=list(use[\"y\"][-7:])\n",
    "\n",
    "   \n",
    " \n",
    "  resultn,sman,smap=ne_prophet(train_data,test_data,len(data)-7,\"D\")\n",
    "  result.append(resultn)\n",
    "  test.append(test_data)\n",
    "  \n",
    "  smape_list.append(smap)\n",
    "  smape_list_np.append(sman)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= pd.DataFrame({'name':name,'ne_mape':smape_list,'ne_smape':smape_list_np})\n",
    "res.to_csv('D:/timeseries/result/temp.csv')\n",
    "res2= pd.DataFrame({'name':name,'day1/7_pred':result[:][0], 'day1/7_actu':test[:,0], 'day2/7_pred':result[:,1], 'day2/7_actu':test[:,1],\n",
    "                       'day3/7_pred':result[:][2], 'day3/7_actu':test[:,2], 'day4/7_pred':result[:,3], 'day4/7_actu':test[:,3], \n",
    "                       'day5/7_pred':result[:,4], 'day5/7_actu':test[:,4], 'day6/7_pred':result[:,5], 'day6/7_actu':test[:,5],\n",
    "                       'day7/7_pred':result[:,6], 'day7/7_actu':test[:,6]\n",
    "                       })\n",
    "res2.to_csv('D:/timeseries/result/pred_2_np.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=data.columns[1:-5]\n",
    "dat=data[name]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newrun(df,j,name,ep):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  X = df[name][:-1].values\n",
    "  Y = df[name][1:].values\n",
    "\n",
    "\n",
    "  X_norm = X / X[0] \n",
    "  Y_norm = Y / Y[0]\n",
    "\n",
    "\n",
    "  X_train, Y_train = X_norm[:-j], Y_norm[:-j]\n",
    "  X_test, Y_test = X_norm[-j:], Y_norm[-j:]\n",
    "  X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "  \n",
    "\n",
    "\n",
    "  model = Sequential()\n",
    " \n",
    "  model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "  model.add(Dense(X_train.shape[2]))  \n",
    "  model.compile(loss='mae', optimizer='adam')\n",
    "  # print(X_train,\"tra\")  \n",
    "\n",
    " \n",
    "  history = model.fit(X_train, Y_train, epochs=ep, batch_size=32, validation_data=(X_test, Y_test), verbose=2, shuffle=False)\n",
    "  \n",
    "  # for i in range(1,len(name)):\n",
    "  X_pred=np.array(df.loc[[len(df)-1],name])\n",
    "\n",
    "    # X_pred=np.concatenate(X_pred,df[name[i]].values[-1])\n",
    "\n",
    " \n",
    "  X_pred_norm = X_pred / X[0]\n",
    "  Y_pred_norm = model.predict(np.array([X_pred_norm]))\n",
    "  Y_pred = Y_pred_norm * X[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  train_predict = model.predict(X_train)\n",
    "  test_predict = model.predict(X_test)\n",
    "\n",
    " \n",
    "  smpl=[]\n",
    "  mpl=[]\n",
    "  pred=[]\n",
    "  test=[]\n",
    "\n",
    "  for i in range(200):\n",
    "    # plt.figure(figsize=(10,6))\n",
    "  \n",
    "    # plt.plot(df['ds'][-j:], Y_test[:,i]*X[0][i], label='Test Actual')\n",
    "    # plt.plot(df['ds'][-j:], test_predict[:,i]*X[0][i], label='Test Predicted')\n",
    "    mp=lstmmape(Y_test[:,i]*X[0][i],test_predict[:,i]*X[0][i])\n",
    "    smp=lstmsmape(Y_test[:,i]*X[0][i],test_predict[:,i]*X[0][i])\n",
    "    pred.append(test_predict[:,i]*X[0][i])\n",
    "    test.append(Y_test[:,i]*X[0][i])\n",
    "   \n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Price of %s'%name[i])\n",
    "    # plt.title('Price of %s Prediction'%name[i])\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    mpl.append(mp)\n",
    "    smpl.append(smp)\n",
    "    # print(mpl,smpl,'adas')\n",
    "    # mpl.append(mp)\n",
    "\n",
    "\n",
    "\n",
    "  return smpl,mpl,pred,test\n",
    "\n",
    "def lstmsmape(A, F):\n",
    "  \n",
    "  A=list(A)\n",
    "  F=list(F)\n",
    "  # print(A,\"A\",\"f\",F)\n",
    "  # A=A[:-1]\n",
    "  # F=F[:-1]\n",
    " \n",
    "  \n",
    "  sum=0\n",
    "  for i in range(len(A)):\n",
    "    sum+=2 * np.abs(F[i] - A[i]) / (np.abs(A[i]) + np.abs(F[i]))\n",
    "    # print(sum,\"as\")\n",
    "\n",
    "  return 100/len(A) * sum\n",
    "\n",
    "def lstmmape(A,F):\n",
    "  sum=0\n",
    "  A=list(A)\n",
    "  F=list(F)\n",
    "  # print(A,\"A\",\"f\",F)\n",
    "  # A=A[:-1]\n",
    "  # F=F[:-1]\n",
    "  \n",
    "  for i in range(len(A)):\n",
    "   \n",
    "\n",
    "    sum+=abs(A[i] - F[i]) / abs(A[i])\n",
    "  return 100/len(A)*sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=['ds', 'DJUBSWH_x', 'GMECN09', 'DJUBCST', 'SWIX23H', 'GMESC15',\n",
    "       'NPN416E', 'NPN206E', 'CDOM2UM', 'WTLM3UM', 'NPFL09E', 'SGENCHT',\n",
    "       'EEXHR19', 'SPGPGHE', 'SGGYEXR', 'DCIENTR', 'GMESA10', 'NPS404E',\n",
    "       'OIL45SC', 'RJEFNET', 'LCP15MT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/11 - 3s - loss: 0.9375 - val_loss: 0.6372 - 3s/epoch - 252ms/step\n",
      "Epoch 2/40\n",
      "11/11 - 0s - loss: 0.6701 - val_loss: 0.3762 - 57ms/epoch - 5ms/step\n",
      "Epoch 3/40\n",
      "11/11 - 0s - loss: 0.3864 - val_loss: 0.2561 - 63ms/epoch - 6ms/step\n",
      "Epoch 4/40\n",
      "11/11 - 0s - loss: 0.2173 - val_loss: 0.2219 - 67ms/epoch - 6ms/step\n",
      "Epoch 5/40\n",
      "11/11 - 0s - loss: 0.1612 - val_loss: 0.2005 - 66ms/epoch - 6ms/step\n",
      "Epoch 6/40\n",
      "11/11 - 0s - loss: 0.1437 - val_loss: 0.1927 - 61ms/epoch - 6ms/step\n",
      "Epoch 7/40\n",
      "11/11 - 0s - loss: 0.1365 - val_loss: 0.1905 - 63ms/epoch - 6ms/step\n",
      "Epoch 8/40\n",
      "11/11 - 0s - loss: 0.1329 - val_loss: 0.1812 - 69ms/epoch - 6ms/step\n",
      "Epoch 9/40\n",
      "11/11 - 0s - loss: 0.1310 - val_loss: 0.1801 - 66ms/epoch - 6ms/step\n",
      "Epoch 10/40\n",
      "11/11 - 0s - loss: 0.1317 - val_loss: 0.1743 - 68ms/epoch - 6ms/step\n",
      "Epoch 11/40\n",
      "11/11 - 0s - loss: 0.1294 - val_loss: 0.1732 - 63ms/epoch - 6ms/step\n",
      "Epoch 12/40\n",
      "11/11 - 0s - loss: 0.1289 - val_loss: 0.1714 - 64ms/epoch - 6ms/step\n",
      "Epoch 13/40\n",
      "11/11 - 0s - loss: 0.1290 - val_loss: 0.1679 - 66ms/epoch - 6ms/step\n",
      "Epoch 14/40\n",
      "11/11 - 0s - loss: 0.1270 - val_loss: 0.1677 - 66ms/epoch - 6ms/step\n",
      "Epoch 15/40\n",
      "11/11 - 0s - loss: 0.1276 - val_loss: 0.1658 - 77ms/epoch - 7ms/step\n",
      "Epoch 16/40\n",
      "11/11 - 0s - loss: 0.1262 - val_loss: 0.1618 - 79ms/epoch - 7ms/step\n",
      "Epoch 17/40\n",
      "11/11 - 0s - loss: 0.1262 - val_loss: 0.1649 - 62ms/epoch - 6ms/step\n",
      "Epoch 18/40\n",
      "11/11 - 0s - loss: 0.1259 - val_loss: 0.1578 - 56ms/epoch - 5ms/step\n",
      "Epoch 19/40\n",
      "11/11 - 0s - loss: 0.1237 - val_loss: 0.1605 - 53ms/epoch - 5ms/step\n",
      "Epoch 20/40\n",
      "11/11 - 0s - loss: 0.1242 - val_loss: 0.1577 - 54ms/epoch - 5ms/step\n",
      "Epoch 21/40\n",
      "11/11 - 0s - loss: 0.1260 - val_loss: 0.1548 - 64ms/epoch - 6ms/step\n",
      "Epoch 22/40\n",
      "11/11 - 0s - loss: 0.1262 - val_loss: 0.1586 - 68ms/epoch - 6ms/step\n",
      "Epoch 23/40\n",
      "11/11 - 0s - loss: 0.1255 - val_loss: 0.1543 - 77ms/epoch - 7ms/step\n",
      "Epoch 24/40\n",
      "11/11 - 0s - loss: 0.1210 - val_loss: 0.1548 - 64ms/epoch - 6ms/step\n",
      "Epoch 25/40\n",
      "11/11 - 0s - loss: 0.1199 - val_loss: 0.1537 - 56ms/epoch - 5ms/step\n",
      "Epoch 26/40\n",
      "11/11 - 0s - loss: 0.1201 - val_loss: 0.1553 - 62ms/epoch - 6ms/step\n",
      "Epoch 27/40\n",
      "11/11 - 0s - loss: 0.1187 - val_loss: 0.1534 - 71ms/epoch - 6ms/step\n",
      "Epoch 28/40\n",
      "11/11 - 0s - loss: 0.1187 - val_loss: 0.1575 - 63ms/epoch - 6ms/step\n",
      "Epoch 29/40\n",
      "11/11 - 0s - loss: 0.1163 - val_loss: 0.1567 - 67ms/epoch - 6ms/step\n",
      "Epoch 30/40\n",
      "11/11 - 0s - loss: 0.1190 - val_loss: 0.1550 - 74ms/epoch - 7ms/step\n",
      "Epoch 31/40\n",
      "11/11 - 0s - loss: 0.1168 - val_loss: 0.1567 - 70ms/epoch - 6ms/step\n",
      "Epoch 32/40\n",
      "11/11 - 0s - loss: 0.1191 - val_loss: 0.1580 - 61ms/epoch - 6ms/step\n",
      "Epoch 33/40\n",
      "11/11 - 0s - loss: 0.1181 - val_loss: 0.1583 - 61ms/epoch - 6ms/step\n",
      "Epoch 34/40\n",
      "11/11 - 0s - loss: 0.1132 - val_loss: 0.1623 - 66ms/epoch - 6ms/step\n",
      "Epoch 35/40\n",
      "11/11 - 0s - loss: 0.1129 - val_loss: 0.1583 - 67ms/epoch - 6ms/step\n",
      "Epoch 36/40\n",
      "11/11 - 0s - loss: 0.1138 - val_loss: 0.1634 - 60ms/epoch - 5ms/step\n",
      "Epoch 37/40\n",
      "11/11 - 0s - loss: 0.1132 - val_loss: 0.1645 - 56ms/epoch - 5ms/step\n",
      "Epoch 38/40\n",
      "11/11 - 0s - loss: 0.1118 - val_loss: 0.1681 - 57ms/epoch - 5ms/step\n",
      "Epoch 39/40\n",
      "11/11 - 0s - loss: 0.1092 - val_loss: 0.1672 - 54ms/epoch - 5ms/step\n",
      "Epoch 40/40\n",
      "11/11 - 0s - loss: 0.1071 - val_loss: 0.1623 - 58ms/epoch - 5ms/step\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "smapel=[]\n",
    "mapel=[]\n",
    " \n",
    "datamain=data\n",
    "# for j in range(1):\n",
    "#     epochmp=[]\n",
    "#     epochsmp=[]\n",
    "#     for q in range(4):\n",
    "  \n",
    "      # nam=['ds']\n",
    "      # for t in range(5):\n",
    "         \n",
    "      \n",
    "      #   nam.append(name[1+q*5+t])\n",
    "      # print(nam)\n",
    "    \n",
    "      # datamain=dat[nam]\n",
    "      \n",
    "  \n",
    "        \n",
    "smp,mp,pred,test=newrun(datamain,1,name,40)\n",
    "      # print(smp,mp)\n",
    "      # for k in smp:\n",
    "      #   epochsmp.append(k)\n",
    "      # for k in mp:\n",
    "      #   epochmp.append(k)\n",
    "      # print(epochmp,epochsmp,\"dd\")\n",
    "    # break\n",
    "        \n",
    "\n",
    "    # mapel.append(epochmp)\n",
    "    # smapel.append(epochsmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalpred=[]\n",
    "for i in range(1):\n",
    "    newpred=[j[i] for j in pred]\n",
    "    totalpred.append(newpred)\n",
    "\n",
    "totaltest=[]\n",
    "for i in range(1):\n",
    "    newtest=[j[i] for j in test]\n",
    "    totaltest.append(newtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= pd.DataFrame({'lstm_mape':mp,'lstm_smape':smp})\n",
    "result.to_csv('D:/timeseries/result/temp_2.csv')\n",
    "res2= pd.DataFrame({'name':name,'day1/1_pred':totalpred[0], 'day1/1_actu':totaltest[0]})\n",
    "res2.to_csv('D:/timeseries/result/temp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
